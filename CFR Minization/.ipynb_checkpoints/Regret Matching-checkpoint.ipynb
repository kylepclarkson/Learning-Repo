{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regret Matching with Rock, Paper, Scissors\n",
    "---\n",
    "### Regret\n",
    "For a group of players let $s_i$ be the action player by player $i$ and $s_{-i}$ the actions played by the remaining players.\n",
    "Together, these actions form an action profile $a \\in A$.\n",
    "For all other actions $s_i'$ we could've played, we define the **regret of not playing action $s'_i$** as a difference in utility:  \n",
    "\n",
    "<center>\n",
    "$\n",
    "\\begin{align*}\n",
    "    regret(s_i', a) &= u(s_i', s_{i-1}) - u(a)\n",
    "\\end{align*}\n",
    "$\n",
    "</center>  \n",
    "\n",
    "For example, if we play scissors and our opponent plays rock, then $a=(scissors, rock)$ and our utility for this play is $u(scissors, rock) = -1$.\n",
    "We can compute the regret for all of our possible actions to find:  \n",
    "<center>\n",
    "$\n",
    "\\begin{align*}\n",
    "    regret(rock, a) &= u(rock, rock) - u((scissors, rock)) = 0 - (-1) = 1 \\\\\n",
    "    regret(paper, a) &= u(paper, rock) - u((scissors, rock)) = 1 - (-1) = 2 \\\\\n",
    "    regret(scissors, a) &= u(scissors, rock) - u((scissors, rock)) = -1 - (-1) = 0\n",
    "\\end{align*}\n",
    "$  \n",
    "</center>  \n",
    "\n",
    "Thus we regret not playing \"paper\" the most, and regret not playing \"rock\" more than playing \"scissors\".Note that when $s_i' = s_i$ the regret is zero. \n",
    "\n",
    "---\n",
    "### Regret Matching\n",
    "\n",
    "Actions that have positive regret is an indicator that we should've chosen these actions to maximize our utility. \n",
    "Thus if we track the regret for each action, if we choose actions at random with probability proportional to how positive their regret is, we can hopefully maximize our utility. Actions with negative regret are given zero probability.\n",
    "Such a weighting is called **regret matching**.  \n",
    "\n",
    "If we play another game, using the above regrets we play \"paper\" with probability $\\frac{2}{3}$ and \"rock\" with probability $\\frac{1}{3}$. Suppose we play \"paper\" while our opponent plays \"scissors\". \n",
    "Thus our regret for this game is:\n",
    "<center>\n",
    "$\n",
    "\\begin{align*}\n",
    "    regret(rock, a) &= u(rock, paper) - u((paper, scissors)) = -1 - (-1) = 0 \\\\\n",
    "    regret(paper, a) &= u(paper, paper) - u((paper, scissors)) = 0 - (-1) = 1 \\\\\n",
    "    regret(scissors, a) &= u(scissors, paper) - u((paper, scissors)) = 1 - (-1) = 2\n",
    "\\end{align*}\n",
    "$  \n",
    "</center>  \n",
    "\n",
    "If we add these regrets to our previous regrets, we can compute the **cumulative regrets** of $(1,3,2)$ respectively, which is normalized to $(\\frac{1}{6},\\frac{3}{6},\\frac{2}{6})$.\n",
    "These normalized weights form a mixed-strategy that can be used for the next game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Example RPS:\n",
    "We given an example of regret matching against an opponent that plays rock slightly more than paper or scissors.\n",
    "We initialize the cumulative regret of our agent to 0.\n",
    "Over several iterations, our agent will choose rock, paper, or scissors with a probability proportional to their cummulative regret via a strategy porfile. We then update their regrets using this action and repeat.  \n",
    "\n",
    "Regret Matching Algorithm:  \n",
    "`  \n",
    "Initialize cummulative regret to 0\n",
    "For some number of training iterations:\n",
    "    - Use cummulative regret to define strategy profile\n",
    "    - Add strategy profile to cummulative strategy profile (will use average after training.)\n",
    "    - Agent selects action according to strategy profile.\n",
    "    - Compute agent's regret, given opponents action.\n",
    "    - Update cummulative regret.\n",
    "Normalize cummulative strategy profile by number of training iterations.\n",
    "Return normalized strategy profile. \n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPS:\n",
    "    \n",
    "    ROCK, PAPER, SCISSORS = 0, 1, 2\n",
    "    N_ACTIONS = 3\n",
    "    # Payoff matrix, First index is our agent's choice, Second the opponents choice\n",
    "    util = np.array([[0, -1, 1], [1, 0, -1], [-1, 1, 0]])\n",
    "\n",
    "    def __init__(self, oppo_strat=np.array([1/3, 1/3, 1/3])):\n",
    "        self.reset(oppo_strat)\n",
    "        \n",
    "    def reset(self, oppo_strat):\n",
    "        self.regret_sum = np.array([0.0, 0.0, 0.0])\n",
    "        self.strategy_sum = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "        self.agent_strategy = np.array([0.0, 0.0, 0.0])\n",
    "        self.oppo_strategy = oppo_strat\n",
    "        \n",
    "    def set_agent_strategy(self):\n",
    "        # Set agent's strategy proportional to current regret, or uniform if all are non positive.\n",
    "        normalize_factor = 0\n",
    "        for a in range(self.N_ACTIONS):\n",
    "            self.agent_strategy[a] = self.regret_sum[a] if self.regret_sum[a] > 0 else 0\n",
    "            normalize_factor += self.agent_strategy[a]\n",
    "            \n",
    "        for a in range(self.N_ACTIONS):\n",
    "            if (normalize_factor > 0):\n",
    "                self.agent_strategy[a] /= normalize_factor\n",
    "            else:\n",
    "                self.agent_strategy[a] = 1 / self.N_ACTIONS\n",
    "                \n",
    "    def update_running_strategy(self):\n",
    "        self.strategy_sum += self.agent_strategy\n",
    "        \n",
    "    def get_action(self, strategy):\n",
    "        \n",
    "        # Given strategy, generate rdnum in [0,1], return action corresponding to bin this number falls into. \n",
    "        r = rd.uniform(0,1)\n",
    "\n",
    "        if r < strategy[0]:\n",
    "            return self.ROCK\n",
    "        elif r < strategy[1]+strategy[0]:\n",
    "            return self.PAPER\n",
    "        else:\n",
    "            return self.SCISSORS\n",
    "        \n",
    "    def print_arrays(self):\n",
    "        print(f'Regret sum: {self.regret_sum}')\n",
    "        print(f'Strategy sum: {self.strategy_sum}')\n",
    "        print(f'Agent Strategy: {self.agent_strategy}')\n",
    "        \n",
    "        \n",
    "    def train(self, n_epochs):\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            self.set_agent_strategy()\n",
    "            self.update_running_strategy()\n",
    "            agent_action = self.get_action(self.agent_strategy)\n",
    "            oppo_action = self.get_action(self.oppo_strategy)\n",
    "            for i in range(self.N_ACTIONS):\n",
    "                self.regret_sum[i] += (self.util[i][oppo_action] - self.util[agent_action][oppo_action])\n",
    "#             print(self.regret_sum)\n",
    "        \n",
    "            if epoch % 500 == 0:\n",
    "                \n",
    "                x = self.strategy_sum / epoch\n",
    "                print(self.strategy_sum / epoch)\n",
    "#                 print(f'Epoch {epoch}. Average strategy: {x}')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "rps = RPS([0.8, 0.1, 0.1])\n",
    "rps.train(25_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regret sum: [17647 35204     0]\n",
      "Strategy sum: [0 0 0]\n",
      "Agent Strategy: [0 0 0]\n"
     ]
    }
   ],
   "source": [
    "rps.print_arrays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
